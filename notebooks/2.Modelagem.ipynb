{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if \"notebooks\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "    print(f\"Changed working directory to {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from beer_color_prediction import config\n",
    "\n",
    "\n",
    "def slugify(s):\n",
    "    return re.sub(r\"\\W+\", \"_\", s).lower().strip(\"_\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(config.RAW_DATA_DIR / \"dataset.csv\")\n",
    "df = df.drop(columns=[\"Unnamed: 0\", \"Date/Time\"])\n",
    "df = df.set_index(\"Job ID\")\n",
    "df.columns = [slugify(col) for col in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do problema é entender quais variáveis influenciam na cor da cerveja Amstel, para isso, separamos de antemão o subconjunto em dados da Heineken e da Amstel. Como a coloração não pode ser negativa e há presença de valores faltantes, removemos essas amostras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"color\"])\n",
    "df = df.query(\"color >= 0\")\n",
    "df_amstel = df.query(\"product == 'AMST'\")\n",
    "df_heineken = df.query(\"product == 'HNK'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amstel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heineken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos de antemão um teste (gold) contendo apenas amostras da Amstel e um treino (train) contendo amostras de ambas marcas. Esse subconjunto será utilizado nas análises da predição do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_amstel.sample(frac=0.2, random_state=42)\n",
    "df_amstel = df_amstel.drop(df_test.index)\n",
    "df = df.drop(df_test.index)\n",
    "df_test.shape, df_amstel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    data: pd.DataFrame,\n",
    "    handle_negative_values: str = \"keep\",\n",
    "    handle_outliers: str = \"keep\",\n",
    "    outlier_threshold: float = 1.5,\n",
    "    lower_percentile: float = 0.05,\n",
    "    upper_percentile: float = 0.95,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Preprocesses the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame.\n",
    "        handle_negative_values (str, optional): How to handle negative values. Defaults to \"keep\".\n",
    "        Options: \"keep\", \"replace_with_zero\", \"replace_with_nan\", \"drop\".\n",
    "        handle_outliers (str, optional): How to handle outliers. Defaults to \"keep\".\n",
    "        Options: \"keep\", \"clip\", \"replace_with_nan\", \"drop\".\n",
    "        outlier_threshold (float, optional): Threshold for outlier detection. Defaults to 1.5.\n",
    "        lower_percentile (float, optional): Lower percentile for outlier detection. Defaults to 0.05.\n",
    "        upper_percentile (float, optional): Upper percentile for outlier detection. Defaults to 0.95.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.drop(columns=[\"product\", \"roast_color\"], errors=\"ignore\")\n",
    "\n",
    "    if handle_negative_values == \"replace_with_zero\":\n",
    "        data = data.clip(lower=0)\n",
    "    elif handle_negative_values == \"replace_with_nan\":\n",
    "        data = data.where(data >= 0)\n",
    "    elif handle_negative_values == \"drop\":\n",
    "        data = data[(data >= 0).all(axis=1)]\n",
    "\n",
    "    if (\n",
    "        handle_outliers == \"clip\"\n",
    "        or handle_outliers == \"replace_with_nan\"\n",
    "        or handle_outliers == \"drop\"\n",
    "    ):\n",
    "        iqrs = data.quantile(upper_percentile) - data.quantile(lower_percentile)\n",
    "        lower_bound = data.quantile(lower_percentile) - outlier_threshold * iqrs\n",
    "        upper_bound = data.quantile(upper_percentile) + outlier_threshold * iqrs\n",
    "        if handle_outliers == \"clip\":\n",
    "            data = data.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "        elif handle_outliers == \"replace_with_nan\":\n",
    "            data = data.where((data >= lower_bound) & (data <= upper_bound))\n",
    "        elif handle_outliers == \"drop\":\n",
    "            data = data[((data >= lower_bound) & (data <= upper_bound)).all(axis=1)]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return mse, rmse, r2, mae, mape\n",
    "\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    mse, rmse, r2, mae, mape = evaluate(y_true, y_pred)\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R^2: {r2:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}\")\n",
    "\n",
    "\n",
    "def filter_df_by_product(df_in, product_name):\n",
    "    \"\"\"Filters a DataFrame by a specific product name.\n",
    "\n",
    "    Args:\n",
    "      df: The input DataFrame.\n",
    "      product_name: The name of the product to filter by.\n",
    "\n",
    "    Returns:\n",
    "      A new DataFrame containing only rows with the specified product.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_df = (\n",
    "        df_in.join(df[\"product\"])\n",
    "        .query(f\"product == '{product_name}'\")\n",
    "        .drop(columns=[\"product\"])\n",
    "    )\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = preprocess_data(\n",
    "    df, handle_negative_values=\"keep\", handle_outliers=\"keep\"\n",
    ")  # Não mudar nada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df1.drop(columns=[\"color\"]), df1[\"color\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(\n",
    "    X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_val.shape, y_val.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 1: Sem tratamento de valores extremos e negativos\n",
    "Para um primeiro teste, não removemos os valores extremos ou negativos e usamos uma estratégia simples de preenchimento de valores faltantes. Para isso, usamos a média dos valores da coluna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy Regressor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = TransformedTargetRegressor(\n",
    "    regressor=DummyRegressor(strategy=\"mean\"),\n",
    "    func=np.log,\n",
    "    inverse_func=np.exp,\n",
    ")\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Predições:\")\n",
    "print_metrics(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"\\nAMSTEL\")\n",
    "test_amstel = filter_df_by_product(X_test, \"AMST\")\n",
    "y_pred = regressor.predict(test_amstel)\n",
    "print_metrics(y_test.loc[test_amstel.index], y_pred)\n",
    "\n",
    "print(\"\\nHeineken\")\n",
    "test_heineken = filter_df_by_product(X_test, \"HNK\")\n",
    "y_pred = regressor.predict(test_heineken)\n",
    "print_metrics(y_test.loc[test_heineken.index], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testar múltiplos modelos\n",
    "\n",
    "Usamos o LazyPredict para treinar diversos modelos de regressão e escolher o melhor para a tarefa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, predictions = reg.fit(X_train, X_val, y_train, y_val)\n",
    "\n",
    "clear_output()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O melhor modelo segundo o LazyPredict foi ExtraTreesRegressor, no entanto, ao utilizar o modelo para fazer previsões, o resultado r2 se mostrou muito baixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = models.head(3)\n",
    "\n",
    "print(\"Predições:\\n\\n\")\n",
    "for model_name in top_models.index:\n",
    "    y_pred = reg.models[model_name].predict(X_test)\n",
    "    print(\"-\" * 5, model_name, \"-\" * 5)\n",
    "    print(\"\\nGeral:\")\n",
    "    print_metrics(y_test, y_pred)\n",
    "\n",
    "    print(\"\\nAMSTEL\")\n",
    "    test_amstel = filter_df_by_product(X_test, \"AMST\")\n",
    "    y_pred = reg.models[model_name].predict(test_amstel)\n",
    "    print_metrics(y_test.loc[test_amstel.index], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 2: Com tratamento de valores extremos e negativos\n",
    "\n",
    "Na exploração de dados identificamos que existem valores extremos e negativos que podem estar prejudicando o treinamento dos modelos. Para isso, vamos tratar esses valores e treinar os modelos novamente. Os valores faltantes estão sendo preenchidos com a média dos valores da coluna pelo LazyPrediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nessa estratégia removemos as linhas com valores negativos e outliers\n",
    "_X_train = preprocess_data(\n",
    "    X_train,\n",
    "    handle_negative_values=\"drop\",\n",
    "    handle_outliers=\"drop\",\n",
    ")\n",
    "\n",
    "_X_test = preprocess_data(\n",
    "    X_test,\n",
    "    handle_negative_values=\"drop\",\n",
    "    handle_outliers=\"drop\",\n",
    ")\n",
    "\n",
    "_X_val = preprocess_data(\n",
    "    X_val,\n",
    "    handle_negative_values=\"drop\",\n",
    "    handle_outliers=\"drop\",\n",
    ")\n",
    "\n",
    "_y_train = y_train.loc[_X_train.index]\n",
    "_y_test = y_test.loc[_X_test.index]\n",
    "_y_val = y_val.loc[_X_val.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy Regressor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = TransformedTargetRegressor(\n",
    "    regressor=DummyRegressor(strategy=\"mean\"),\n",
    "    func=np.log,\n",
    "    inverse_func=np.exp,\n",
    ")\n",
    "\n",
    "regressor.fit(_X_train, _y_train)\n",
    "\n",
    "_y_pred = regressor.predict(_X_test)\n",
    "\n",
    "print_metrics(_y_test, _y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testar múltiplos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=mean_absolute_error)\n",
    "\n",
    "models, predictions = reg.fit(_X_train, _X_val, _y_train, _y_val)\n",
    "\n",
    "clear_output()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = models.head(3)\n",
    "\n",
    "print(\"Predições:\\n\\n\")\n",
    "for model_name in top_models.index:\n",
    "    y_pred = reg.models[model_name].predict(_X_test)\n",
    "    print(\"-\" * 5, model_name, \"-\" * 5)\n",
    "    print(\"\\nGeral:\")\n",
    "    print_metrics(_y_test, y_pred)\n",
    "\n",
    "    print(\"\\nAMSTEL\")\n",
    "    test_amstel = filter_df_by_product(_X_test, \"AMST\")\n",
    "    y_pred = reg.models[model_name].predict(test_amstel)\n",
    "    print_metrics(_y_test.loc[test_amstel.index], y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazer a remoção dos valores extremos ajudou no r2 e rmse do modelo ExtraTreesRegressor, mas ao remover esses valores nosso conjunto de dados foi reduzido significativamente. A seguir, não vamos remover os valores extremos, mas substituít-los pela média dos valores da coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ao substituir os valores negativos e outliers por NaN, o simples imputer vai substituir esses valores pela média\n",
    "\n",
    "strategy = \"replace_with_nan\"\n",
    "_X_train = preprocess_data(\n",
    "    X_train,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "\n",
    "_X_test = preprocess_data(\n",
    "    X_test,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "\n",
    "_X_val = preprocess_data(\n",
    "    X_val,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "\n",
    "_y_train = y_train.loc[_X_train.index]\n",
    "_y_test = y_test.loc[_X_test.index]\n",
    "_y_val = y_val.loc[_X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=mean_absolute_error)\n",
    "\n",
    "models, predictions = reg.fit(_X_train, _X_val, _y_train, _y_val)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = models.head(3)\n",
    "for model_name in top_models.index:\n",
    "    y_pred = reg.models[model_name].predict(_X_test)\n",
    "    print(\"-\" * 5, model_name, \"-\" * 5)\n",
    "    print(\"\\nGeral:\")\n",
    "    print_metrics(_y_test, y_pred)\n",
    "\n",
    "    print(\"\\nAMSTEL\")\n",
    "    test_amstel = filter_df_by_product(_X_test, \"AMST\")\n",
    "    y_pred = reg.models[model_name].predict(test_amstel)\n",
    "    print_metrics(_y_test.loc[test_amstel.index], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As métricas foram piores comparada a remoção no geral. Substituir os outliers usando a estratégia de clipping não melhorou o modelo comparado a remoção dos outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"clip\"\n",
    "_X_train = preprocess_data(\n",
    "    X_train,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "\n",
    "_X_test = preprocess_data(\n",
    "    X_test,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "\n",
    "_X_val = preprocess_data(\n",
    "    X_val,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "\n",
    "_y_train = y_train.loc[_X_train.index]\n",
    "_y_test = y_test.loc[_X_test.index]\n",
    "_y_val = y_val.loc[_X_val.index]\n",
    "\n",
    "reg = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=mean_absolute_error)\n",
    "\n",
    "models, predictions = reg.fit(_X_train, _X_val, _y_train, _y_val)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = models.head(3)\n",
    "for model_name in top_models.index:\n",
    "    y_pred = reg.models[model_name].predict(_X_test)\n",
    "    print(\"-\" * 5, model_name, \"-\" * 5)\n",
    "    print(\"\\nGeral:\")\n",
    "    print_metrics(_y_test, y_pred)\n",
    "\n",
    "    print(\"\\nAMSTEL\")\n",
    "    test_amstel = filter_df_by_product(_X_test, \"AMST\")\n",
    "    y_pred = reg.models[model_name].predict(test_amstel)\n",
    "    print_metrics(_y_test.loc[test_amstel.index], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 3: Busca de normalizador e estratégias de preenchimento dos dados\n",
    "\n",
    "O modelo final escolhido foi o ExtraTreesRegressor, pois foi o que apresentou \"melhores\" métricas. A seguir, vamos testar diferentes estratégias de tratamento de valores faltantes para tentar melhorar o modelo. Primeiro substituímos os valores extremos/negativos por nan com o propósito de testar diferentes abordagens de preenchimento desses valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_train = preprocess_data(\n",
    "    X_train,\n",
    "    handle_negative_values=\"replace_with_nan\",\n",
    "    handle_outliers=\"replace_with_nan\",\n",
    ")\n",
    "\n",
    "_X_test = preprocess_data(\n",
    "    X_test,\n",
    "    handle_negative_values=\"replace_with_nan\",\n",
    "    handle_outliers=\"replace_with_nan\",\n",
    ")\n",
    "\n",
    "_X_val = preprocess_data(\n",
    "    X_val,\n",
    "    handle_negative_values=\"replace_with_nan\",\n",
    "    handle_outliers=\"replace_with_nan\",\n",
    ")\n",
    "\n",
    "_y_train = y_train.loc[_X_train.index]\n",
    "_y_test = y_test.loc[_X_test.index]\n",
    "_y_val = y_val.loc[_X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    imputer = trial.suggest_categorical(\"imputer\", [\"simple\", \"knn\", \"iterative\"])\n",
    "    scaler = trial.suggest_categorical(\"scaler\", [\"standard\", \"minmax\", \"robust\"])\n",
    "    target_transformer = trial.suggest_categorical(\n",
    "        \"target_transformer\", [\"log\", \"minmax\", \"robust\", \"standard\", \"none\"]\n",
    "    )\n",
    "\n",
    "    if imputer == \"knn\":\n",
    "        n_neighbors = trial.suggest_int(\"n_neighbors\", 3, 10)\n",
    "        weight = trial.suggest_categorical(\"weight\", [\"uniform\", \"distance\"])\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors, weights=weight)\n",
    "    elif imputer == \"iterative\":\n",
    "        max_iter = trial.suggest_int(\"max_iter\", 5, 20)\n",
    "        imputer = IterativeImputer(max_iter=max_iter, random_state=42)\n",
    "    elif imputer == \"simple\":\n",
    "        strategy = trial.suggest_categorical(\n",
    "            \"strategy\", [\"mean\", \"median\", \"most_frequent\"]\n",
    "        )\n",
    "        imputer = SimpleImputer(strategy=strategy)\n",
    "\n",
    "    if scaler == \"standard\":\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    elif scaler == \"minmax\":\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    elif scaler == \"robust\":\n",
    "        scaler = preprocessing.RobustScaler()\n",
    "\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 25, 100)\n",
    "    regressor = ExtraTreesRegressor(n_estimators=n_estimators, random_state=42)\n",
    "\n",
    "    regressor = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", imputer),\n",
    "            (\"scaler\", scaler),\n",
    "            (\"regressor\", regressor),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if target_transformer == \"log\":\n",
    "        regressor = TransformedTargetRegressor(\n",
    "            regressor=regressor,\n",
    "            func=np.log,\n",
    "            inverse_func=np.exp,\n",
    "        )\n",
    "    elif target_transformer == \"minmax\":\n",
    "        regressor = TransformedTargetRegressor(\n",
    "            regressor=regressor,\n",
    "            transformer=preprocessing.MinMaxScaler(),\n",
    "        )\n",
    "    elif target_transformer == \"robust\":\n",
    "        regressor = TransformedTargetRegressor(\n",
    "            regressor=regressor,\n",
    "            transformer=preprocessing.RobustScaler(),\n",
    "        )\n",
    "    elif target_transformer == \"standard\":\n",
    "        regressor = TransformedTargetRegressor(\n",
    "            regressor=regressor,\n",
    "            transformer=preprocessing.StandardScaler(),\n",
    "        )\n",
    "\n",
    "    regressor.fit(_X_train, _y_train)\n",
    "    y_pred = regressor.predict(_X_val)\n",
    "    return root_mean_squared_error(_y_val, y_pred)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alterar o tipo de preenchimento dos valores faltantes, o normalizador não influenciou significativamente nas métricas do modelo de acordo com os logs do Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = study.best_trial\n",
    "trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = ExtraTreesRegressor(random_state=42)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", imputer),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"regressor\", reg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = TransformedTargetRegressor(\n",
    "    regressor=pipe,\n",
    "    transformer=preprocessing.RobustScaler(),\n",
    ")\n",
    "\n",
    "pipe.fit(_X_train, _y_train)\n",
    "\n",
    "_y_pred = pipe.predict(_X_test)\n",
    "\n",
    "print_metrics(_y_test, _y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outros Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentos extras em que exploramos aumentar os dados artificialmente e discretizar as variáveis de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construção e seleção de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentar com construcoes de features a partir das existente utilizando operadores basicos: soma, subtracao multiplicacao, etc, de pares de features. Por causa da grande quantidade de features geradas, e necessario uma etapa de selecao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer()),\n",
    "        (\"scaler\", preprocessing.StandardScaler()),\n",
    "        (\"regressor\", ExtraTreesRegressor(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = TransformedTargetRegressor(\n",
    "    regressor=pipe,\n",
    "    transformer=preprocessing.RobustScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_train = preprocess_data(\n",
    "    X_train,\n",
    "    handle_negative_values=\"replace_with_nan\",\n",
    "    handle_outliers=\"replace_with_nan\",\n",
    ")\n",
    "\n",
    "_X_test = preprocess_data(\n",
    "    X_test,\n",
    "    handle_negative_values=\"replace_with_nan\",\n",
    "    handle_outliers=\"replace_with_nan\",\n",
    ")\n",
    "\n",
    "_X_val = preprocess_data(\n",
    "    X_val,\n",
    "    handle_negative_values=\"replace_with_nan\",\n",
    "    handle_outliers=\"replace_with_nan\",\n",
    ")\n",
    "\n",
    "_y_train = y_train.loc[_X_train.index]\n",
    "_y_test = y_test.loc[_X_test.index]\n",
    "_y_val = y_val.loc[_X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature construction\n",
    "# Golden Features\n",
    "# A partir de cada par de features originais, cria uma nova feature usando operadores matemáticos: +, -, /, * e avalia seu poder preditivo.\n",
    "def get_golden_features(data):\n",
    "    golden_features = pd.DataFrame()\n",
    "    for i, col1 in enumerate(data.columns):\n",
    "        for j, col2 in enumerate(data.columns):\n",
    "            if i < j:\n",
    "                golden_features[f\"{col1}_plus_{col2}\"] = data[col1] + data[col2]\n",
    "                golden_features[f\"{col1}_minus_{col2}\"] = data[col1] - data[col2]\n",
    "                golden_features[f\"{col1}_times_{col2}\"] = data[col1] * data[col2]\n",
    "                golden_features[f\"{col1}_div_{col2}\"] = data[col1] / data[col2]\n",
    "    return golden_features\n",
    "\n",
    "_X_train = get_golden_features(_X_train)\n",
    "_X_val = get_golden_features(_X_val)\n",
    "_X_test = get_golden_features(_X_test)\n",
    "\n",
    "_X_train.shape, _X_val.shape, _X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#lets fix ValueError: Input X contains infinity or a value too large for dtype('float64').\n",
    "_X_train = _X_train.replace([np.inf, -np.inf], np.nan)\n",
    "_X_val = _X_val.replace([np.inf, -np.inf], np.nan)\n",
    "_X_test = _X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "_X_train = _X_train.fillna(_X_train.mean())\n",
    "_X_val = _X_val.fillna(_X_val.mean())\n",
    "_X_test = _X_test.fillna(_X_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection: pegar as 100 melhores features\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=100)\n",
    "selector.fit(_X_train,_y_train)\n",
    "_X_train = selector.transform(_X_train)\n",
    "_X_val = selector.transform(_X_val)\n",
    "_X_test = selector.transform(_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estrategia nao obteve ganhos significantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(_X_train, _y_train)\n",
    "y_pred = pipe.predict(_X_test)\n",
    "print_metrics(_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discretização das variáveis de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliamos a performance ao discretizar as variaveis de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer()),\n",
    "        (\"scaler\", preprocessing.StandardScaler()),\n",
    "        (\"regressor\", ExtraTreesRegressor(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = TransformedTargetRegressor(\n",
    "    regressor=pipe,\n",
    "    transformer=preprocessing.RobustScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_train = preprocess_data(\n",
    "    X_train,\n",
    "    handle_negative_values=\"drop\",\n",
    "    handle_outliers=\"drop\",\n",
    ")\n",
    "\n",
    "_X_test = preprocess_data(\n",
    "    X_test,\n",
    "    handle_negative_values=\"drop\",\n",
    "    handle_outliers=\"replace_dropwith_nan\",\n",
    ")\n",
    "\n",
    "_X_val = preprocess_data(\n",
    "    X_val,\n",
    "    handle_negative_values=\"drop\",\n",
    "    handle_outliers=\"drop\",\n",
    ")\n",
    "\n",
    "_y_train = y_train.loc[_X_train.index]\n",
    "_y_test = y_test.loc[_X_test.index]\n",
    "_y_val = y_val.loc[_X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_train.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos discretizar os dados substituindo os valores pelo bin ao qual pertencem. Usaremos o KBinsDiscretizer do scikit-learn.\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=30, encode=\"ordinal\", strategy=\"uniform\")\n",
    "\n",
    "_X_train = discretizer.fit_transform(_X_train)\n",
    "_X_val = discretizer.transform(_X_val)\n",
    "_X_test = discretizer.transform(_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem ganhos significativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(_X_train, _y_train)\n",
    "y_pred = pipe.predict(_X_test)\n",
    "print_metrics(_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Resultados\n",
    "\n",
    "Como a escolha de estratégia de preenchimento de valores faltantes, normalização e tratamento de valores extremos e negativos, entre outros experimentos, não influenciaram significativamente nas métricas do modelo, uma possível melhoria seria a coleta de mais dados para treinamento do modelo. Usamos a configuração padrão do modelo ExtraTreesRegressor, preenchimento de valores faltantes com a média dos valores da coluna e o normalizador StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer()),\n",
    "        (\"scaler\", preprocessing.StandardScaler()),\n",
    "        (\"regressor\", ExtraTreesRegressor(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = TransformedTargetRegressor(\n",
    "    regressor=pipe,\n",
    "    transformer=preprocessing.RobustScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando usando amstel e heineken\n",
    "\n",
    "Recapitulando: O dataset separado no início do notebook contém apenas dados da Amstel. \n",
    "\n",
    "Desejamos saber o desempenho do modelo para estimar a cor da marca, para isso, treinamos o modelo usando os dados de ambas as marcas, vemos que as métricas foram similares as obtidas nas análises anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"replace_with_nan\"\n",
    "df_train = preprocess_data(\n",
    "    df,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "\n",
    "X_train = df_train.drop(columns=[\"color\"])\n",
    "y_train = df_train[\"color\"]\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "df_test = preprocess_data(\n",
    "    df_test,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "X_test = df_test.drop(columns=[\"color\"])\n",
    "y_test = df_test[\"color\"]\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "regressor = TransformedTargetRegressor(\n",
    "    regressor=reg,\n",
    "    func=np.log,\n",
    "    inverse_func=np.exp,\n",
    ")\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tree_model = pipe.regressor_[\"regressor\"]\n",
    "feature_importance = extra_tree_model.feature_importances_\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(np.array(X_train.columns)[sorted_idx], feature_importance[sorted_idx], color=\"#00561F\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar parte das features não mudou significativamente as métricas do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_features = X_train.columns[sorted_idx][:10]\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer()),\n",
    "        (\"scaler\", preprocessing.StandardScaler()),\n",
    "        (\"regressor\", ExtraTreesRegressor(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = TransformedTargetRegressor(\n",
    "    regressor=pipe,\n",
    "    transformer=preprocessing.RobustScaler(),\n",
    ")\n",
    "\n",
    "pipe.fit(X_train[most_important_features], y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test[most_important_features])\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando usando apenas Amstel\n",
    "\n",
    "As métricas apresentaram pouca variação em relação ao treinamento com os dados de ambas as marcas. Então, treinar o modelo com uma ou com as duas marcas não influenciou significativamente nas métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"replace_with_nan\"\n",
    "df_train = preprocess_data(\n",
    "    df_amstel,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "\n",
    "X_train = df_train.drop(columns=[\"color\"])\n",
    "y_train = df_train[\"color\"]\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "df_test = preprocess_data(\n",
    "    df_test,\n",
    "    handle_negative_values=strategy,\n",
    "    handle_outliers=strategy,\n",
    ")\n",
    "X_test = df_test.drop(columns=[\"color\"])\n",
    "y_test = df_test[\"color\"]\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print_metrics(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
